/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
LR: [0.0004000007974766396]
---- EP [1/5] | BTCH [1/3634] ||| train_loss = 0.01536 ----
LR: [0.000400003189906295]
---- EP [1/5] | BTCH [2/3634] ||| train_loss = 0.01050 ----
LR: [0.0004000071772881711]
---- EP [1/5] | BTCH [3/3634] ||| train_loss = 0.00824 ----
LR: [0.0004000127596209425]
---- EP [1/5] | BTCH [4/3634] ||| train_loss = 0.01452 ----
LR: [0.0004000199369027565]
---- EP [1/5] | BTCH [5/3634] ||| train_loss = 0.01649 ----
LR: [0.0004000287091312244]
---- EP [1/5] | BTCH [6/3634] ||| train_loss = 0.00489 ----
LR: [0.00040003907630343366]
---- EP [1/5] | BTCH [7/3634] ||| train_loss = 0.02118 ----
LR: [0.00040005103841593906]
---- EP [1/5] | BTCH [8/3634] ||| train_loss = 0.01092 ----
LR: [0.0004000645954647646]
---- EP [1/5] | BTCH [9/3634] ||| train_loss = 0.00900 ----
LR: [0.00040007974744540875]
---- EP [1/5] | BTCH [10/3634] ||| train_loss = 0.01432 ----
LR: [0.0004000964943528338]
---- EP [1/5] | BTCH [11/3634] ||| train_loss = 0.01675 ----
LR: [0.0004001148361814765]
---- EP [1/5] | BTCH [12/3634] ||| train_loss = 0.01111 ----
LR: [0.0004001347729252411]
---- EP [1/5] | BTCH [13/3634] ||| train_loss = 0.01144 ----
LR: [0.0004001563045775043]
---- EP [1/5] | BTCH [14/3634] ||| train_loss = 0.01051 ----
LR: [0.0004001794311311105]
---- EP [1/5] | BTCH [15/3634] ||| train_loss = 0.01875 ----
LR: [0.00040020415257837477]
---- EP [1/5] | BTCH [16/3634] ||| train_loss = 0.02484 ----
LR: [0.000400230468911085]
---- EP [1/5] | BTCH [17/3634] ||| train_loss = 0.01242 ----
LR: [0.00040025838012049293]
---- EP [1/5] | BTCH [18/3634] ||| train_loss = 0.01081 ----
LR: [0.0004002878861973265]
---- EP [1/5] | BTCH [19/3634] ||| train_loss = 0.00829 ----
LR: [0.00040031898713178277]
---- EP [1/5] | BTCH [20/3634] ||| train_loss = 0.01864 ----
LR: [0.0004003516829135245]
---- EP [1/5] | BTCH [21/3634] ||| train_loss = 0.01011 ----
LR: [0.0004003859735316889]
---- EP [1/5] | BTCH [22/3634] ||| train_loss = 0.00870 ----
LR: [0.0004004218589748823]
---- EP [1/5] | BTCH [23/3634] ||| train_loss = 0.01417 ----
LR: [0.0004004593392311802]
---- EP [1/5] | BTCH [24/3634] ||| train_loss = 0.01140 ----
LR: [0.00040049841428812724]
---- EP [1/5] | BTCH [25/3634] ||| train_loss = 0.01646 ----
LR: [0.00040053908413274254]
---- EP [1/5] | BTCH [26/3634] ||| train_loss = 0.00873 ----
LR: [0.0004005813487515091]
---- EP [1/5] | BTCH [27/3634] ||| train_loss = 0.01182 ----
LR: [0.00040062520813038613]
---- EP [1/5] | BTCH [28/3634] ||| train_loss = 0.01206 ----
LR: [0.00040067066225479495]
---- EP [1/5] | BTCH [29/3634] ||| train_loss = 0.00877 ----
LR: [0.0004007177111096383]
---- EP [1/5] | BTCH [30/3634] ||| train_loss = 0.01495 ----
LR: [0.0004007663546792793]
---- EP [1/5] | BTCH [31/3634] ||| train_loss = 0.01253 ----
LR: [0.0004008165929475539]
---- EP [1/5] | BTCH [32/3634] ||| train_loss = 0.01450 ----
LR: [0.00040086842589776885]
---- EP [1/5] | BTCH [33/3634] ||| train_loss = 0.02006 ----
LR: [0.0004009218535127052]
---- EP [1/5] | BTCH [34/3634] ||| train_loss = 0.01729 ----
LR: [0.0004009768757746047]
---- EP [1/5] | BTCH [35/3634] ||| train_loss = 0.00785 ----
LR: [0.00040103349266518845]
---- EP [1/5] | BTCH [36/3634] ||| train_loss = 0.02083 ----
LR: [0.00040109170416563825]
---- EP [1/5] | BTCH [37/3634] ||| train_loss = 0.01341 ----
LR: [0.0004011515102566171]
---- EP [1/5] | BTCH [38/3634] ||| train_loss = 0.00568 ----
LR: [0.0004012129109182503]
---- EP [1/5] | BTCH [39/3634] ||| train_loss = 0.01901 ----
LR: [0.00040127590613013575]
---- EP [1/5] | BTCH [40/3634] ||| train_loss = 0.01567 ----
LR: [0.00040134049587134055]
---- EP [1/5] | BTCH [41/3634] ||| train_loss = 0.01091 ----
LR: [0.0004014066801204044]
---- EP [1/5] | BTCH [42/3634] ||| train_loss = 0.00971 ----
LR: [0.0004014744588553328]
---- EP [1/5] | BTCH [43/3634] ||| train_loss = 0.01631 ----
LR: [0.0004015438320536073]
---- EP [1/5] | BTCH [44/3634] ||| train_loss = 0.00866 ----
LR: [0.0004016147996921734]
---- EP [1/5] | BTCH [45/3634] ||| train_loss = 0.00700 ----
LR: [0.00040168736174745097]
---- EP [1/5] | BTCH [46/3634] ||| train_loss = 0.00854 ----
LR: [0.0004017615181953309]
---- EP [1/5] | BTCH [47/3634] ||| train_loss = 0.01117 ----
LR: [0.0004018372690111696]
---- EP [1/5] | BTCH [48/3634] ||| train_loss = 0.01380 ----
LR: [0.00040191461416979636]
---- EP [1/5] | BTCH [49/3634] ||| train_loss = 0.01675 ----
LR: [0.0004019935536455129]
---- EP [1/5] | BTCH [50/3634] ||| train_loss = 0.00938 ----
LR: [0.00040207408741209025]
---- EP [1/5] | BTCH [51/3634] ||| train_loss = 0.00722 ----
LR: [0.0004021562154427633]
---- EP [1/5] | BTCH [52/3634] ||| train_loss = 0.01422 ----
LR: [0.00040223993771024667]
---- EP [1/5] | BTCH [53/3634] ||| train_loss = 0.01263 ----
LR: [0.0004023252541867188]
---- EP [1/5] | BTCH [54/3634] ||| train_loss = 0.01686 ----
LR: [0.0004024121648438326]
---- EP [1/5] | BTCH [55/3634] ||| train_loss = 0.01269 ----
LR: [0.00040250066965270663]
---- EP [1/5] | BTCH [56/3634] ||| train_loss = 0.00938 ----
LR: [0.0004025907685839356]
---- EP [1/5] | BTCH [57/3634] ||| train_loss = 0.01142 ----
LR: [0.00040268246160757824]
---- EP [1/5] | BTCH [58/3634] ||| train_loss = 0.00881 ----
LR: [0.00040277574869316754]
---- EP [1/5] | BTCH [59/3634] ||| train_loss = 0.00724 ----
LR: [0.0004028706298097075]
---- EP [1/5] | BTCH [60/3634] ||| train_loss = 0.01238 ----
LR: [0.0004029671049256677]
---- EP [1/5] | BTCH [61/3634] ||| train_loss = 0.00925 ----
LR: [0.00040306517400899404]
---- EP [1/5] | BTCH [62/3634] ||| train_loss = 0.01478 ----
LR: [0.00040316483702709967]
---- EP [1/5] | BTCH [63/3634] ||| train_loss = 0.01102 ----
LR: [0.000403266093946867]
---- EP [1/5] | BTCH [64/3634] ||| train_loss = 0.01199 ----
LR: [0.00040336894473465106]
---- EP [1/5] | BTCH [65/3634] ||| train_loss = 0.00733 ----
LR: [0.00040347338935627433]
---- EP [1/5] | BTCH [66/3634] ||| train_loss = 0.00578 ----
LR: [0.00040357942777703715]
---- EP [1/5] | BTCH [67/3634] ||| train_loss = 0.01153 ----
LR: [0.0004036870599616986]
---- EP [1/5] | BTCH [68/3634] ||| train_loss = 0.01557 ----
LR: [0.00040379628587449735]
---- EP [1/5] | BTCH [69/3634] ||| train_loss = 0.00748 ----
LR: [0.00040390710547913954]
---- EP [1/5] | BTCH [70/3634] ||| train_loss = 0.01268 ----
LR: [0.00040401951873880043]
---- EP [1/5] | BTCH [71/3634] ||| train_loss = 0.01220 ----
LR: [0.0004041335256161297]
---- EP [1/5] | BTCH [72/3634] ||| train_loss = 0.00931 ----
LR: [0.0004042491260732445]
---- EP [1/5] | BTCH [73/3634] ||| train_loss = 0.00893 ----
LR: [0.000404366320071731]
---- EP [1/5] | BTCH [74/3634] ||| train_loss = 0.01034 ----
LR: [0.00040448510757264827]
---- EP [1/5] | BTCH [75/3634] ||| train_loss = 0.01065 ----
LR: [0.00040460548853652606]
---- EP [1/5] | BTCH [76/3634] ||| train_loss = 0.00738 ----
LR: [0.0004047274629233634]
---- EP [1/5] | BTCH [77/3634] ||| train_loss = 0.00966 ----
LR: [0.00040485103069263194]
---- EP [1/5] | BTCH [78/3634] ||| train_loss = 0.02353 ----
LR: [0.0004049761918032708]
---- EP [1/5] | BTCH [79/3634] ||| train_loss = 0.00938 ----
LR: [0.00040510294621368993]
---- EP [1/5] | BTCH [80/3634] ||| train_loss = 0.01205 ----
LR: [0.00040523129388177377]
---- EP [1/5] | BTCH [81/3634] ||| train_loss = 0.01607 ----
LR: [0.0004053612347648741]
---- EP [1/5] | BTCH [82/3634] ||| train_loss = 0.01850 ----
LR: [0.000405492768819812]
---- EP [1/5] | BTCH [83/3634] ||| train_loss = 0.00622 ----
LR: [0.00040562589600288447]
---- EP [1/5] | BTCH [84/3634] ||| train_loss = 0.01392 ----
LR: [0.00040576061626985266]
---- EP [1/5] | BTCH [85/3634] ||| train_loss = 0.02141 ----
LR: [0.000405896929575952]
---- EP [1/5] | BTCH [86/3634] ||| train_loss = 0.01092 ----
LR: [0.00040603483587589063]
---- EP [1/5] | BTCH [87/3634] ||| train_loss = 0.01653 ----
LR: [0.0004061743351238406]
---- EP [1/5] | BTCH [88/3634] ||| train_loss = 0.02022 ----
LR: [0.00040631542727345357]
---- EP [1/5] | BTCH [89/3634] ||| train_loss = 0.00786 ----
LR: [0.00040645811227784344]
---- EP [1/5] | BTCH [90/3634] ||| train_loss = 0.00756 ----
LR: [0.0004066023900895985]
---- EP [1/5] | BTCH [91/3634] ||| train_loss = 0.01448 ----
LR: [0.0004067482606607796]
---- EP [1/5] | BTCH [92/3634] ||| train_loss = 0.01909 ----
LR: [0.0004068957239429169]
---- EP [1/5] | BTCH [93/3634] ||| train_loss = 0.00815 ----
LR: [0.0004070447798870114]
---- EP [1/5] | BTCH [94/3634] ||| train_loss = 0.01431 ----
LR: [0.0004071954284435315]
---- EP [1/5] | BTCH [95/3634] ||| train_loss = 0.01580 ----
LR: [0.00040734766956242]
---- EP [1/5] | BTCH [96/3634] ||| train_loss = 0.01135 ----
LR: [0.00040750150319309245]
---- EP [1/5] | BTCH [97/3634] ||| train_loss = 0.01162 ----
LR: [0.0004076569292844317]
---- EP [1/5] | BTCH [98/3634] ||| train_loss = 0.01440 ----
LR: [0.00040781394778479155]
---- EP [1/5] | BTCH [99/3634] ||| train_loss = 0.01489 ----
LR: [0.0004079725586420002]
---- EP [1/5] | BTCH [100/3634] ||| train_loss = 0.00897 ----
LR: [0.00040813276180334986]
---- EP [1/5] | BTCH [101/3634] ||| train_loss = 0.01166 ----
LR: [0.0004082945572156122]
---- EP [1/5] | BTCH [102/3634] ||| train_loss = 0.00739 ----
LR: [0.00040845794482502126]
---- EP [1/5] | BTCH [103/3634] ||| train_loss = 0.00846 ----
LR: [0.00040862292457728884]
---- EP [1/5] | BTCH [104/3634] ||| train_loss = 0.00792 ----
LR: [0.00040878949641759595]
---- EP [1/5] | BTCH [105/3634] ||| train_loss = 0.01211 ----
LR: [0.00040895766029059276]
---- EP [1/5] | BTCH [106/3634] ||| train_loss = 0.01297 ----
LR: [0.00040912741614040037]
---- EP [1/5] | BTCH [107/3634] ||| train_loss = 0.01791 ----
LR: [0.00040929876391061423]
---- EP [1/5] | BTCH [108/3634] ||| train_loss = 0.00884 ----
LR: [0.00040947170354429553]
---- EP [1/5] | BTCH [109/3634] ||| train_loss = 0.00951 ----
LR: [0.0004096462349839833]
---- EP [1/5] | BTCH [110/3634] ||| train_loss = 0.01024 ----
LR: [0.0004098223581716805]
---- EP [1/5] | BTCH [111/3634] ||| train_loss = 0.01671 ----
LR: [0.0004100000730488662]
---- EP [1/5] | BTCH [112/3634] ||| train_loss = 0.00973 ----
LR: [0.00041017937955648877]
---- EP [1/5] | BTCH [113/3634] ||| train_loss = 0.00707 ----
LR: [0.00041036027763496907]
---- EP [1/5] | BTCH [114/3634] ||| train_loss = 0.00916 ----
LR: [0.0004105427672241955]
---- EP [1/5] | BTCH [115/3634] ||| train_loss = 0.01467 ----
LR: [0.0004107268482635325]
---- EP [1/5] | BTCH [116/3634] ||| train_loss = 0.01860 ----
LR: [0.00041091252069181024]
---- EP [1/5] | BTCH [117/3634] ||| train_loss = 0.00876 ----
LR: [0.0004110997844473368]
---- EP [1/5] | BTCH [118/3634] ||| train_loss = 0.01356 ----
LR: [0.00041128863946788587]
---- EP [1/5] | BTCH [119/3634] ||| train_loss = 0.00823 ----
LR: [0.0004114790856907021]
---- EP [1/5] | BTCH [120/3634] ||| train_loss = 0.01234 ----
LR: [0.000411671123052508]
---- EP [1/5] | BTCH [121/3634] ||| train_loss = 0.01588 ----
LR: [0.0004118647514894918]
---- EP [1/5] | BTCH [122/3634] ||| train_loss = 0.01000 ----
LR: [0.00041205997093731253]
---- EP [1/5] | BTCH [123/3634] ||| train_loss = 0.01020 ----
LR: [0.0004122567813311037]
---- EP [1/5] | BTCH [124/3634] ||| train_loss = 0.01020 ----
LR: [0.0004124551826054663]
---- EP [1/5] | BTCH [125/3634] ||| train_loss = 0.00976 ----
LR: [0.000412655174694479]
---- EP [1/5] | BTCH [126/3634] ||| train_loss = 0.01621 ----
LR: [0.0004128567575316863]
---- EP [1/5] | BTCH [127/3634] ||| train_loss = 0.01304 ----
LR: [0.00041305993105010715]
---- EP [1/5] | BTCH [128/3634] ||| train_loss = 0.01112 ----
LR: [0.00041326469518222775]
---- EP [1/5] | BTCH [129/3634] ||| train_loss = 0.01221 ----
LR: [0.00041347104986001054]
---- EP [1/5] | BTCH [130/3634] ||| train_loss = 0.00970 ----
LR: [0.0004136789950148871]
---- EP [1/5] | BTCH [131/3634] ||| train_loss = 0.02117 ----
LR: [0.00041388853057775996]
---- EP [1/5] | BTCH [132/3634] ||| train_loss = 0.00842 ----
LR: [0.0004140996564790077]
---- EP [1/5] | BTCH [133/3634] ||| train_loss = 0.00991 ----
LR: [0.00041431237264847286]
---- EP [1/5] | BTCH [134/3634] ||| train_loss = 0.01308 ----
LR: [0.00041452667901547766]
---- EP [1/5] | BTCH [135/3634] ||| train_loss = 0.00551 ----
LR: [0.00041474257550880994]
---- EP [1/5] | BTCH [136/3634] ||| train_loss = 0.00813 ----
LR: [0.0004149600620567285]
---- EP [1/5] | BTCH [137/3634] ||| train_loss = 0.00596 ----
LR: [0.00041517913858697165]
---- EP [1/5] | BTCH [138/3634] ||| train_loss = 0.01002 ----
LR: [0.00041539980502674177]
---- EP [1/5] | BTCH [139/3634] ||| train_loss = 0.01045 ----
LR: [0.0004156220613027138]
---- EP [1/5] | BTCH [140/3634] ||| train_loss = 0.01347 ----
LR: [0.00041584590734103886]
---- EP [1/5] | BTCH [141/3634] ||| train_loss = 0.00860 ----
LR: [0.00041607134306733545]
---- EP [1/5] | BTCH [142/3634] ||| train_loss = 0.00826 ----
LR: [0.00041629836840669475]
---- EP [1/5] | BTCH [143/3634] ||| train_loss = 0.01225 ----
LR: [0.00041652698328368057]
---- EP [1/5] | BTCH [144/3634] ||| train_loss = 0.01586 ----
LR: [0.00041675718762233284]
---- EP [1/5] | BTCH [145/3634] ||| train_loss = 0.02185 ----
LR: [0.00041698898134615026]
---- EP [1/5] | BTCH [146/3634] ||| train_loss = 0.00966 ----
LR: [0.00041722236437811977]
---- EP [1/5] | BTCH [147/3634] ||| train_loss = 0.00794 ----
LR: [0.0004174573366406871]
---- EP [1/5] | BTCH [148/3634] ||| train_loss = 0.01931 ----
LR: [0.0004176938980557793]
---- EP [1/5] | BTCH [149/3634] ||| train_loss = 0.01369 ----
LR: [0.0004179320485447873]
---- EP [1/5] | BTCH [150/3634] ||| train_loss = 0.01070 ----
LR: [0.0004181717880285818]
---- EP [1/5] | BTCH [151/3634] ||| train_loss = 0.00933 ----
LR: [0.000418413116427499]
---- EP [1/5] | BTCH [152/3634] ||| train_loss = 0.01322 ----
LR: [0.0004186560336613514]
---- EP [1/5] | BTCH [153/3634] ||| train_loss = 0.01665 ----
LR: [0.00041890053964941873]
---- EP [1/5] | BTCH [154/3634] ||| train_loss = 0.01022 ----
LR: [0.00041914663431046224]
---- EP [1/5] | BTCH [155/3634] ||| train_loss = 0.00664 ----
LR: [0.0004193943175627036]
---- EP [1/5] | BTCH [156/3634] ||| train_loss = 0.01393 ----
LR: [0.0004196435893238457]
---- EP [1/5] | BTCH [157/3634] ||| train_loss = 0.01287 ----
LR: [0.00041989444951105563]
---- EP [1/5] | BTCH [158/3634] ||| train_loss = 0.01338 ----
LR: [0.00042014689804098157]
---- EP [1/5] | BTCH [159/3634] ||| train_loss = 0.01057 ----
LR: [0.0004204009348297375]
---- EP [1/5] | BTCH [160/3634] ||| train_loss = 0.01339 ----
LR: [0.00042065655979291183]
---- EP [1/5] | BTCH [161/3634] ||| train_loss = 0.02243 ----
LR: [0.0004209137728455655]
---- EP [1/5] | BTCH [162/3634] ||| train_loss = 0.00605 ----
LR: [0.0004211725739022322]
---- EP [1/5] | BTCH [163/3634] ||| train_loss = 0.01194 ----
LR: [0.000421432962876913]
---- EP [1/5] | BTCH [164/3634] ||| train_loss = 0.00964 ----
LR: [0.0004216949396830885]
---- EP [1/5] | BTCH [165/3634] ||| train_loss = 0.00594 ----
LR: [0.00042195850423370866]
---- EP [1/5] | BTCH [166/3634] ||| train_loss = 0.01411 ----
LR: [0.0004222236564411959]
---- EP [1/5] | BTCH [167/3634] ||| train_loss = 0.01269 ----
LR: [0.00042249039621744186]
---- EP [1/5] | BTCH [168/3634] ||| train_loss = 0.01497 ----
LR: [0.0004227587234738178]
---- EP [1/5] | BTCH [169/3634] ||| train_loss = 0.00533 ----
LR: [0.0004230286381211607]
---- EP [1/5] | BTCH [170/3634] ||| train_loss = 0.01202 ----
LR: [0.0004233001400697836]
---- EP [1/5] | BTCH [171/3634] ||| train_loss = 0.00627 ----
LR: [0.0004235732292294705]
---- EP [1/5] | BTCH [172/3634] ||| train_loss = 0.00635 ----
LR: [0.0004238479055094814]
---- EP [1/5] | BTCH [173/3634] ||| train_loss = 0.01292 ----
LR: [0.0004241241688185439]
---- EP [1/5] | BTCH [174/3634] ||| train_loss = 0.01100 ----
LR: [0.0004244020190648616]
---- EP [1/5] | BTCH [175/3634] ||| train_loss = 0.00750 ----
LR: [0.0004246814561561091]
---- EP [1/5] | BTCH [176/3634] ||| train_loss = 0.01588 ----
LR: [0.000424962479999437]
---- EP [1/5] | BTCH [177/3634] ||| train_loss = 0.01242 ----
LR: [0.00042524509050146167]
---- EP [1/5] | BTCH [178/3634] ||| train_loss = 0.01521 ----
LR: [0.00042552928756828083]
---- EP [1/5] | BTCH [179/3634] ||| train_loss = 0.00897 ----
LR: [0.0004258150711054596]
---- EP [1/5] | BTCH [180/3634] ||| train_loss = 0.01621 ----
LR: [0.0004261024410180357]
---- EP [1/5] | BTCH [181/3634] ||| train_loss = 0.01159 ----
VAL ||| loss = 0.013139120721380704, psnr = 30.791349411010742, ssim = 0.9086753726005554
LR: [0.0004263913972105231]
---- EP [1/5] | BTCH [182/3634] ||| train_loss = 0.01051 ----
LR: [0.0004266819395869082]
---- EP [1/5] | BTCH [183/3634] ||| train_loss = 0.01863 ----
LR: [0.0004269740680506468]
---- EP [1/5] | BTCH [184/3634] ||| train_loss = 0.01402 ----
LR: [0.00042726778250466897]
---- EP [1/5] | BTCH [185/3634] ||| train_loss = 0.00896 ----
LR: [0.00042756308285138087]
---- EP [1/5] | BTCH [186/3634] ||| train_loss = 0.00712 ----
LR: [0.0004278599689926596]
---- EP [1/5] | BTCH [187/3634] ||| train_loss = 0.01354 ----
LR: [0.0004281584408298532]
---- EP [1/5] | BTCH [188/3634] ||| train_loss = 0.02794 ----
LR: [0.00042845849826378575]
---- EP [1/5] | BTCH [189/3634] ||| train_loss = 0.00920 ----
LR: [0.0004287601411947558]
---- EP [1/5] | BTCH [190/3634] ||| train_loss = 0.00979 ----
LR: [0.000429063369522531]
---- EP [1/5] | BTCH [191/3634] ||| train_loss = 0.01805 ----
LR: [0.00042936818314635167]
---- EP [1/5] | BTCH [192/3634] ||| train_loss = 0.01511 ----
LR: [0.0004296745819649377]
---- EP [1/5] | BTCH [193/3634] ||| train_loss = 0.00910 ----
LR: [0.0004299825658764765]
---- EP [1/5] | BTCH [194/3634] ||| train_loss = 0.01209 ----
LR: [0.0004302921347786315]
---- EP [1/5] | BTCH [195/3634] ||| train_loss = 0.00516 ----
LR: [0.000430603288568537]
---- EP [1/5] | BTCH [196/3634] ||| train_loss = 0.00777 ----
LR: [0.00043091602714280357]
---- EP [1/5] | BTCH [197/3634] ||| train_loss = 0.00834 ----
LR: [0.00043123035039751255]
---- EP [1/5] | BTCH [198/3634] ||| train_loss = 0.00743 ----
LR: [0.00043154625822822146]
---- EP [1/5] | BTCH [199/3634] ||| train_loss = 0.01546 ----
LR: [0.00043186375052996044]
---- EP [1/5] | BTCH [200/3634] ||| train_loss = 0.01668 ----
LR: [0.0004321828271972288]
---- EP [1/5] | BTCH [201/3634] ||| train_loss = 0.01130 ----
LR: [0.00043250348812400724]
---- EP [1/5] | BTCH [202/3634] ||| train_loss = 0.00783 ----
LR: [0.0004328257332037438]
---- EP [1/5] | BTCH [203/3634] ||| train_loss = 0.01521 ----
LR: [0.0004331495623293626]
---- EP [1/5] | BTCH [204/3634] ||| train_loss = 0.01401 ----
LR: [0.0004334749753932606]
---- EP [1/5] | BTCH [205/3634] ||| train_loss = 0.00704 ----
LR: [0.0004338019722873107]
---- EP [1/5] | BTCH [206/3634] ||| train_loss = 0.00801 ----
LR: [0.00043413055290285503]
---- EP [1/5] | BTCH [207/3634] ||| train_loss = 0.01506 ----
LR: [0.0004344607171307153]
---- EP [1/5] | BTCH [208/3634] ||| train_loss = 0.01308 ----
LR: [0.00043479246486118235]
---- EP [1/5] | BTCH [209/3634] ||| train_loss = 0.00979 ----
LR: [0.0004351257959840215]
---- EP [1/5] | BTCH [210/3634] ||| train_loss = 0.00984 ----
LR: [0.0004354607103884741]
---- EP [1/5] | BTCH [211/3634] ||| train_loss = 0.00860 ----
LR: [0.0004357972079632542]
---- EP [1/5] | BTCH [212/3634] ||| train_loss = 0.01055 ----
LR: [0.0004361352885965484]
---- EP [1/5] | BTCH [213/3634] ||| train_loss = 0.00858 ----
LR: [0.00043647495217602125]
---- EP [1/5] | BTCH [214/3634] ||| train_loss = 0.01772 ----
LR: [0.0004368161985888047]
---- EP [1/5] | BTCH [215/3634] ||| train_loss = 0.00615 ----
LR: [0.00043715902772151205]
---- EP [1/5] | BTCH [216/3634] ||| train_loss = 0.01027 ----
LR: [0.0004375034394602257]
---- EP [1/5] | BTCH [217/3634] ||| train_loss = 0.01037 ----
LR: [0.0004378494336905042]
---- EP [1/5] | BTCH [218/3634] ||| train_loss = 0.01264 ----
LR: [0.0004381970102973823]
---- EP [1/5] | BTCH [219/3634] ||| train_loss = 0.01566 ----
LR: [0.00043854616916536374]
---- EP [1/5] | BTCH [220/3634] ||| train_loss = 0.01083 ----
LR: [0.00043889691017842854]
---- EP [1/5] | BTCH [221/3634] ||| train_loss = 0.01067 ----
LR: [0.0004392492332200345]
---- EP [1/5] | BTCH [222/3634] ||| train_loss = 0.01895 ----
LR: [0.0004396031381731103]
---- EP [1/5] | BTCH [223/3634] ||| train_loss = 0.01511 ----
LR: [0.0004399586249200574]
---- EP [1/5] | BTCH [224/3634] ||| train_loss = 0.01079 ----
LR: [0.00044031569334275836]
---- EP [1/5] | BTCH [225/3634] ||| train_loss = 0.01255 ----
LR: [0.0004406743433225616]
---- EP [1/5] | BTCH [226/3634] ||| train_loss = 0.01467 ----
LR: [0.00044103457474029516]
---- EP [1/5] | BTCH [227/3634] ||| train_loss = 0.01013 ----
LR: [0.00044139638747626303]
---- EP [1/5] | BTCH [228/3634] ||| train_loss = 0.01515 ----
LR: [0.0004417597814102385]
---- EP [1/5] | BTCH [229/3634] ||| train_loss = 0.00925 ----
LR: [0.0004421247564214744]
---- EP [1/5] | BTCH [230/3634] ||| train_loss = 0.01079 ----
LR: [0.0004424913123886945]
---- EP [1/5] | BTCH [231/3634] ||| train_loss = 0.01204 ----
LR: [0.00044285944919010035]
---- EP [1/5] | BTCH [232/3634] ||| train_loss = 0.01578 ----
LR: [0.00044322916670336623]
---- EP [1/5] | BTCH [233/3634] ||| train_loss = 0.00894 ----
LR: [0.000443600464805639]
---- EP [1/5] | BTCH [234/3634] ||| train_loss = 0.01570 ----
LR: [0.00044397334337354863]
---- EP [1/5] | BTCH [235/3634] ||| train_loss = 0.00531 ----
LR: [0.0004443478022831907]
---- EP [1/5] | BTCH [236/3634] ||| train_loss = 0.01481 ----
LR: [0.0004447238414101388]
---- EP [1/5] | BTCH [237/3634] ||| train_loss = 0.00854 ----
LR: [0.00044510146062944414]
---- EP [1/5] | BTCH [238/3634] ||| train_loss = 0.00816 ----
LR: [0.0004454806598156273]
---- EP [1/5] | BTCH [239/3634] ||| train_loss = 0.01148 ----
LR: [0.00044586143884269014]
---- EP [1/5] | BTCH [240/3634] ||| train_loss = 0.01139 ----
LR: [0.0004462437975841071]
---- EP [1/5] | BTCH [241/3634] ||| train_loss = 0.01559 ----
LR: [0.00044662773591282526]
---- EP [1/5] | BTCH [242/3634] ||| train_loss = 0.01648 ----
LR: [0.00044701325370126964]
---- EP [1/5] | BTCH [243/3634] ||| train_loss = 0.01372 ----
LR: [0.00044740035082133957]
---- EP [1/5] | BTCH [244/3634] ||| train_loss = 0.01862 ----
LR: [0.00044778902714440877]
---- EP [1/5] | BTCH [245/3634] ||| train_loss = 0.00850 ----
LR: [0.00044817928254133055]
---- EP [1/5] | BTCH [246/3634] ||| train_loss = 0.01093 ----
LR: [0.0004485711168824274]
---- EP [1/5] | BTCH [247/3634] ||| train_loss = 0.01156 ----
LR: [0.0004489645300374979]
---- EP [1/5] | BTCH [248/3634] ||| train_loss = 0.01180 ----
LR: [0.00044935952187582194]
---- EP [1/5] | BTCH [249/3634] ||| train_loss = 0.01756 ----
LR: [0.0004497560922661469]
---- EP [1/5] | BTCH [250/3634] ||| train_loss = 0.00546 ----
LR: [0.0004501542410767032]
---- EP [1/5] | BTCH [251/3634] ||| train_loss = 0.01713 ----
LR: [0.0004505539681751939]
---- EP [1/5] | BTCH [252/3634] ||| train_loss = 0.01407 ----
LR: [0.0004509552734287912]
---- EP [1/5] | BTCH [253/3634] ||| train_loss = 0.01280 ----
LR: [0.0004513581567041538]
---- EP [1/5] | BTCH [254/3634] ||| train_loss = 0.00863 ----
LR: [0.00045176261786740966]
---- EP [1/5] | BTCH [255/3634] ||| train_loss = 0.00774 ----
LR: [0.00045216865678416283]
---- EP [1/5] | BTCH [256/3634] ||| train_loss = 0.01168 ----
LR: [0.0004525762733194952]
---- EP [1/5] | BTCH [257/3634] ||| train_loss = 0.01356 ----
LR: [0.00045298546733796123]
---- EP [1/5] | BTCH [258/3634] ||| train_loss = 0.00968 ----
LR: [0.0004533962387035951]
---- EP [1/5] | BTCH [259/3634] ||| train_loss = 0.00874 ----
LR: [0.0004538085872799035]
---- EP [1/5] | BTCH [260/3634] ||| train_loss = 0.01856 ----
LR: [0.0004542225129298711]
---- EP [1/5] | BTCH [261/3634] ||| train_loss = 0.01343 ----
LR: [0.00045463801551595684]
---- EP [1/5] | BTCH [262/3634] ||| train_loss = 0.01013 ----
LR: [0.0004550550949000993]
---- EP [1/5] | BTCH [263/3634] ||| train_loss = 0.00544 ----
LR: [0.00045547375094370617]
---- EP [1/5] | BTCH [264/3634] ||| train_loss = 0.00560 ----
LR: [0.00045589398350767]
---- EP [1/5] | BTCH [265/3634] ||| train_loss = 0.00724 ----
LR: [0.0004563157924523543]
---- EP [1/5] | BTCH [266/3634] ||| train_loss = 0.01259 ----
LR: [0.00045673917763759675]
---- EP [1/5] | BTCH [267/3634] ||| train_loss = 0.00982 ----
LR: [0.0004571641389227183]
---- EP [1/5] | BTCH [268/3634] ||| train_loss = 0.01079 ----
LR: [0.00045759067616650724]
---- EP [1/5] | BTCH [269/3634] ||| train_loss = 0.00999 ----
LR: [0.00045801878922723666]
---- EP [1/5] | BTCH [270/3634] ||| train_loss = 0.01065 ----
LR: [0.0004584484779626523]
